# services/shared/utils/simple_rag.py

import os
import logging
import numpy as np
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
from sqlalchemy.orm import Session
from sqlalchemy import text

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç –Ω–∞ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π
try:
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç –∏–∑ shared (Docker)
    from shared.models.document import Document, DocumentChunk
except ImportError:
    try:
        # Fallback –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
        from models.document import Document, DocumentChunk
    except ImportError:
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback
        from services.shared.models.document import Document, DocumentChunk

try:
    from .llm_client import SimpleLLMClient, LLMResponse
except ImportError:
    from llm_client import SimpleLLMClient, LLMResponse

logger = logging.getLogger(__name__)

class SimpleRAG:
    """
    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–∞—è RAG —Å–∏—Å—Ç–µ–º–∞
    - –õ–æ–∫–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
    - GigaChat –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
    - –ù–∏–∫–∞–∫–∏—Ö —Å–ª–æ–∂–Ω–æ—Å—Ç–µ–π!
    """
    
    def __init__(self, db_session: Session, gigachat_api_key: str):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã
        
        Args:
            db_session: –°–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            gigachat_api_key: API –∫–ª—é—á –¥–ª—è GigaChat
        """
        self.db_session = db_session
        self.llm_client = SimpleLLMClient(gigachat_api_key)
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.similarity_threshold = float(os.getenv("SIMILARITY_THRESHOLD", "0.3"))
        self.search_limit = int(os.getenv("SEARCH_LIMIT", "15"))
        self.min_similarity = float(os.getenv("MIN_SIMILARITY_THRESHOLD", "0.25"))
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–≥–µ—Ä
        self.logger = logging.getLogger(__name__)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        self.embedding_model = SentenceTransformer('cointegrated/rubert-tiny2')
        self.logger.info(f"–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –ù–∞—Å—Ç—Ä–æ–π–∫–∏: similarity_threshold={self.similarity_threshold}, search_limit={self.search_limit}, min_similarity={self.min_similarity}")
        
    def create_embedding(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        try:
            embedding = self.embedding_model.encode([text])[0]
            return embedding.tolist()
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return []
    
    def _format_embedding_for_pgvector(self, embedding: List[float]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å pgvector"""
        return str(embedding).replace(' ', '')
    
    def search_relevant_chunks(self, question: str, limit: int = 15) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            # 1. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞
            question_embedding = self.create_embedding(question)
            self.logger.info(f"–°–æ–∑–¥–∞–Ω —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {len(question_embedding)}")
            
            # 2. –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –∑–∞—Ä–ø–ª–∞—Ç–µ
            salary_keywords = ['–∑–∞—Ä–ø–ª–∞—Ç–∞', '–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞', '–≤—ã–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è', '–≤—ã–ø–ª–∞—Ç–∞', '–æ–ø–ª–∞—Ç–∞ —Ç—Ä—É–¥–∞']
            is_salary_question = any(keyword in question.lower() for keyword in salary_keywords)
            
            if is_salary_question:
                self.logger.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω –≤–æ–ø—Ä–æ—Å –æ –∑–∞—Ä–ø–ª–∞—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É –ø–æ–∏—Å–∫–∞")
                
                # –ò—â–µ–º —á–∞–Ω–∫–∏ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏ –≤—ã–ø–ª–∞—Ç
                salary_query = text("""
                    SELECT dc.id, dc.document_id, dc.chunk_index, dc.content,
                           1 - (dc.embedding_vector <=> :embedding) as similarity,
                           dc.content_length
                    FROM document_chunks dc
                    JOIN documents d ON dc.document_id = d.id
                    WHERE d.processing_status = 'completed'
                      AND dc.embedding_vector IS NOT NULL
                      AND (dc.content ILIKE '%12%' AND dc.content ILIKE '%27%' AND dc.content ILIKE '%–≤—ã–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è%')
                    ORDER BY dc.embedding_vector <=> :embedding
                    LIMIT 3
                """)
                
                salary_result = self.db_session.execute(salary_query, {
                    'embedding': self._format_embedding_for_pgvector(question_embedding)
                })
                
                salary_chunks = []
                for row in salary_result:
                    if row.similarity > 0.3:  # –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
                        salary_chunks.append({
                            'id': row.id,
                            'document_id': row.document_id,
                            'chunk_index': row.chunk_index,
                            'content': row.content,
                            'similarity': row.similarity + 0.2,  # –ë–æ–Ω—É—Å –∑–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å
                            'search_type': 'salary_specific',
                            'content_length': row.content_length
                        })
                
                if salary_chunks:
                    self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(salary_chunks)} —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –æ –∑–∞—Ä–ø–ª–∞—Ç–µ")
            
            # 3. –í—ã–ø–æ–ª–Ω—è–µ–º –æ–±—ã—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            self.logger.info("–í—ã–ø–æ–ª–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫...")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º pgvector –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            query = text("""
                SELECT dc.id, dc.document_id, dc.chunk_index, dc.content,
                       1 - (dc.embedding_vector <=> :embedding) as similarity,
                       dc.content_length
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.processing_status = 'completed'
                  AND dc.embedding_vector IS NOT NULL
                  AND dc.content_length > 100
                  AND dc.content_length < 4000
                  AND dc.content NOT ILIKE '%–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ%'
                  AND dc.content NOT ILIKE '%—É—Ç–≤–µ—Ä–∂–¥–∞—é%'
                  AND dc.content NOT ILIKE '%–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä%'
                  AND dc.content NOT ILIKE '%—Å–∏—Å—Ç–µ–º–∞ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞%'
                  AND dc.content NOT ILIKE '%–≤–≤–µ–¥–µ–Ω–æ –≤–ø–µ—Ä–≤—ã–µ%'
                  AND dc.content NOT ILIKE '%–¥–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è%'
                ORDER BY dc.embedding_vector <=> :embedding
                LIMIT :limit
            """)
            
            result = self.db_session.execute(query, {
                'embedding': self._format_embedding_for_pgvector(question_embedding),
                'limit': limit * 2
            })
            
            vector_chunks = []
            for row in result:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
                if (row.similarity > self.min_similarity and  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –≤–º–µ—Å—Ç–æ 0.25
                    self._is_relevant_content(row.content, question)):
                    vector_chunks.append({
                        'id': row.id,
                        'document_id': row.document_id,
                        'chunk_index': row.chunk_index,
                        'content': row.content,
                        'similarity': row.similarity,
                        'search_type': 'vector',
                        'content_length': row.content_length
                    })
            
            self.logger.info(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(vector_chunks)} –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            
            # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            all_chunks = []
            
            # –°–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —á–∞–Ω–∫–∏ –æ –∑–∞—Ä–ø–ª–∞—Ç–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if is_salary_question and 'salary_chunks' in locals():
                all_chunks.extend(salary_chunks)
            
            # –ó–∞—Ç–µ–º –¥–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∏—Å–∫–ª—é—á–∞—è –¥—É–±–ª–∏–∫–∞—Ç—ã)
            existing_ids = {chunk['id'] for chunk in all_chunks}
            for chunk in vector_chunks:
                if chunk['id'] not in existing_ids:
                    all_chunks.append(chunk)
            
            # 5. –î–æ–ø–æ–ª–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø–æ–∏—Å–∫–æ–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if len(all_chunks) < limit:
                text_chunks = []
                keywords = self._extract_keywords(question)
                
                if keywords:
                    self.logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {keywords}")
                    
                    # –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    conditions = []
                    params = {}
                    
                    for i, keyword in enumerate(keywords[:5]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                        param_name = f'keyword_{i}'
                        conditions.append(f"dc.content ILIKE :{param_name}")
                        params[param_name] = f'%{keyword}%'
                    
                    text_query = text(f"""
                        SELECT dc.id, dc.document_id, dc.chunk_index, dc.content,
                                dc.content_length
                        FROM document_chunks dc
                        JOIN documents d ON dc.document_id = d.id
                        WHERE d.processing_status = 'completed'
                          AND dc.content_length > 200
                          AND dc.content_length < 3000
                          AND dc.content NOT ILIKE '%–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ%'
                          AND dc.content NOT ILIKE '%—É—Ç–≤–µ—Ä–∂–¥–∞—é%'
                          AND dc.content NOT ILIKE '%–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä%'
                          AND dc.content NOT ILIKE '%—Å–∏—Å—Ç–µ–º–∞ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞%'
                          AND dc.content NOT ILIKE '%–ø–æ–ª–æ–∂–µ–Ω–∏–µ%–æ%'
                          AND dc.content NOT ILIKE '%–≤–≤–µ–¥–µ–Ω–æ –≤–ø–µ—Ä–≤—ã–µ%'
                          AND dc.content NOT ILIKE '%–¥–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è%'
                          AND ({' OR '.join(conditions)})
                        ORDER BY dc.content_length DESC
                        LIMIT :limit
                    """)
                    
                    params['limit'] = limit
                    text_result = self.db_session.execute(text_query, params)
                    
                    existing_ids = {chunk['id'] for chunk in all_chunks}
                    for row in text_result:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ—Ç —á–∞–Ω–∫ –µ—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                        if (row.id not in existing_ids and
                            self._is_relevant_content(row.content, question)):
                            
                            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                            content_words = set(row.content.lower().split())
                            question_words = set(question.lower().split())
                            overlap = len(content_words & question_words)
                            
                            # –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞—Å—á–µ—Ç similarity
                            base_similarity = 0.3
                            keyword_bonus = 0.0
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
                            for keyword in keywords:
                                if keyword.lower() in row.content.lower():
                                    keyword_bonus += 0.1
                            
                            # –ë–æ–Ω—É—Å –∑–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
                            word_bonus = min(overlap * 0.05, 0.3)
                            
                            calculated_similarity = base_similarity + keyword_bonus + word_bonus
                            calculated_similarity = min(calculated_similarity, 0.95)  # –ú–∞–∫—Å–∏–º—É–º 0.95
                            
                            if overlap >= 1:  # –ú–∏–Ω–∏–º—É–º 1 –æ–±—â–µ–µ —Å–ª–æ–≤–æ
                                text_chunks.append({
                                    'id': row.id,
                                    'document_id': row.document_id,
                                    'chunk_index': row.chunk_index,
                                    'content': row.content,
                                    'similarity': calculated_similarity,
                                    'search_type': 'text',
                                    'content_length': row.content_length
                                })
                    
                    all_chunks.extend(text_chunks)
                    self.logger.info(f"–¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(text_chunks)} –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
            
            # 6. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ (—É–±—ã–≤–∞–Ω–∏–µ)
            all_chunks.sort(key=lambda x: x['similarity'], reverse=True)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_chunks = all_chunks[:limit]
            
            if not final_chunks:
                self.logger.info("–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                return self._fallback_search(question, limit)
            
            return final_chunks
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ search_relevant_chunks: {str(e)}")
            return self._fallback_search(question, limit)
    
    def _extract_keywords(self, question: str) -> List[str]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –Ω–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ —Å –Ω–æ–≤—ã–º–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
        synonyms = {
            # HR –∏ –∑–∞—Ä–ø–ª–∞—Ç–∞
            '–∞–≤–∞–Ω—Å': ['–∞–≤–∞–Ω—Å', '–∞–≤–∞–Ω—Å–æ–≤–∞—è', '–∞–≤–∞–Ω—Å–æ–≤—ã–π', '–ø–µ—Ä–≤–∞—è —á–∞—Å—Ç—å', '–ø–µ—Ä–≤–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞', '–ø—Ä–µ–¥–æ–ø–ª–∞—Ç–∞'],
            '–∑–∞—Ä–ø–ª–∞—Ç–∞': ['–∑–∞—Ä–ø–ª–∞—Ç–∞', '–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞', '–æ–ø–ª–∞—Ç–∞ —Ç—Ä—É–¥–∞', '–≤–æ–∑–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ', '–∑–ø', '–¥–æ—Ö–æ–¥'],
            '–≤—ã–ø–ª–∞—Ç–∞': ['–≤—ã–ø–ª–∞—Ç–∞', '–≤—ã–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è', '–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ', '–Ω–∞—á–∏—Å–ª–µ–Ω–∏–µ', '–≤—ã–¥–∞—á–∞', '–ø–ª–∞—Ç–µ–∂'],
            '–¥–∞—Ç–∞': ['–¥–∞—Ç–∞', '—á–∏—Å–ª–æ', '—Å—Ä–æ–∫', '–≤—Ä–µ–º—è', '–∫–æ–≥–¥–∞', '–¥–µ–Ω—å', '–ø–µ—Ä–∏–æ–¥'],
            '—Ä–∞–∑–º–µ—Ä': ['—Ä–∞–∑–º–µ—Ä', '—Å—É–º–º–∞', '–ø—Ä–æ—Ü–µ–Ω—Ç', '—Å–∫–æ–ª—å–∫–æ', '–≤–µ–ª–∏—á–∏–Ω–∞', '–æ–±—ä–µ–º'],
            '–æ—Ç–ø—É—Å–∫': ['–æ—Ç–ø—É—Å–∫', '–æ—Ç–ø—É—Å–∫–Ω—ã–µ', '–æ—Ç–¥—ã—Ö', '–∫–∞–Ω–∏–∫—É–ª—ã', 'vacation'],
            '–±–æ–ª—å–Ω–∏—á–Ω—ã–π': ['–±–æ–ª—å–Ω–∏—á–Ω—ã–π', '–±–æ–ª–µ–∑–Ω—å', '–Ω–µ—Ç—Ä—É–¥–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å', '–ª–∏—Å—Ç –Ω–µ—Ç—Ä—É–¥–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏'],
            '–ø—Ä–µ–º–∏—è': ['–ø—Ä–µ–º–∏—è', '–±–æ–Ω—É—Å', '–ø–æ–æ—â—Ä–µ–Ω–∏–µ', '–Ω–∞–¥–±–∞–≤–∫–∞', '—Å—Ç–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ'],
            '–¥–æ–≥–æ–≤–æ—Ä': ['–¥–æ–≥–æ–≤–æ—Ä', '–∫–æ–Ω—Ç—Ä–∞–∫—Ç', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '—Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä'],
            '—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ': ['—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ', '—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ', '–ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∏–µ', '—É—Ö–æ–¥', 'dismissal'],
            '–≥—Ä–∞—Ñ–∏–∫': ['–≥—Ä–∞—Ñ–∏–∫', '—Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ', '—Ä–µ–∂–∏–º', '–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã', '—Å–º–µ–Ω–∞'],
            '–¥–æ–∫—É–º–µ–Ω—Ç—ã': ['–¥–æ–∫—É–º–µ–Ω—Ç—ã', '—Å–ø—Ä–∞–≤–∫–∏', '–±—É–º–∞–≥–∏', '—Ñ–æ—Ä–º—ã', '–∑–∞—è–≤–ª–µ–Ω–∏—è'],
            
            # –¢–µ—Ö–Ω–∏–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å': ['–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '–æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞', '—Ç–µ—Ö–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '–æ—Ö—Ä–∞–Ω–∞', '–∑–∞—â–∏—Ç–∞'],
            '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è': ['–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '–ø—Ä–∞–≤–∏–ª–∞', '–ø–æ—Ä—è–¥–æ–∫', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞', '—Ä–µ–≥–ª–∞–º–µ–Ω—Ç'],
            '—Å—Ä–µ–¥—Å—Ç–≤–∞_–∑–∞—â–∏—Ç—ã': ['—Å–∏–∑', '—Å—Ä–µ–¥—Å—Ç–≤–∞ –∑–∞—â–∏—Ç—ã', '—Å–ø–µ—Ü–æ–¥–µ–∂–¥–∞', '–∫–∞—Å–∫–∞', '–ø–µ—Ä—á–∞—Ç–∫–∏', '–æ—á–∫–∏'],
            '–Ω–µ—Å—á–∞—Å—Ç–Ω—ã–π_—Å–ª—É—á–∞–π': ['–Ω–µ—Å—á–∞—Å—Ç–Ω—ã–π —Å–ª—É—á–∞–π', '—Ç—Ä–∞–≤–º–∞', '–ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏–µ', '–∞–≤–∞—Ä–∏—è', '–∏–Ω—Ü–∏–¥–µ–Ω—Ç'],
            '–æ–±—É—á–µ–Ω–∏–µ_–±—Ç': ['–æ–±—É—á–µ–Ω–∏–µ', '–∏–Ω—Å—Ç—Ä—É–∫—Ç–∞–∂', '–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞', '–∫—É—Ä—Å—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏'],
            '–º–µ–¥–æ—Å–º–æ—Ç—Ä': ['–º–µ–¥–æ—Å–º–æ—Ç—Ä', '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –æ—Å–º–æ—Ç—Ä', '–¥–∏—Å–ø–∞–Ω—Å–µ—Ä–∏–∑–∞—Ü–∏—è', '–∑–¥–æ—Ä–æ–≤—å–µ'],
            '–ø–æ–∂–∞—Ä–Ω–∞—è_–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å': ['–ø–æ–∂–∞—Ä', '–æ–≥–Ω–µ—Ç—É—à–∏—Ç–µ–ª—å', '—ç–≤–∞–∫—É–∞—Ü–∏—è', '–ø–æ–∂–∞—Ä–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å'],
            
            # IT –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
            '–∫–æ–º–ø—å—é—Ç–µ—Ä': ['–∫–æ–º–ø—å—é—Ç–µ—Ä', '–ø–∫', '–Ω–æ—É—Ç–±—É–∫', '—Ä–∞–±–æ—á–µ–µ –º–µ—Å—Ç–æ', '—Ç–µ—Ö–Ω–∏–∫–∞'],
            '–ø–∞—Ä–æ–ª—å': ['–ø–∞—Ä–æ–ª—å', '–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è', '–¥–æ—Å—Ç—É–ø', '–ª–æ–≥–∏–Ω', '—É—á–µ—Ç–Ω–∞—è –∑–∞–ø–∏—Å—å'],
            '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç': ['–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', '—Å–µ—Ç—å', 'wifi', '–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ', '–æ–Ω–ª–∞–π–Ω'],
            '–ø–æ—á—Ç–∞': ['–ø–æ—á—Ç–∞', 'email', '–µ–º–µ–π–ª', '—ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –ø–æ—á—Ç–∞', '–º–µ–π–ª'],
            '–ø—Ä–æ–≥—Ä–∞–º–º—ã': ['–ø—Ä–æ–≥—Ä–∞–º–º—ã', '—Å–æ—Ñ—Ç', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è', 'software', '—Å–∏—Å—Ç–µ–º–∞'],
            '–¥–∞–Ω–Ω—ã–µ': ['–¥–∞–Ω–Ω—ã–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '—Ñ–∞–π–ª—ã', '–¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç', '–∞—Ä—Ö–∏–≤'],
            '–≤–∏—Ä—É—Å': ['–≤–∏—Ä—É—Å', '–∞–Ω—Ç–∏–≤–∏—Ä—É—Å', 'malware', '–∑–∞—â–∏—Ç–∞', '—É–≥—Ä–æ–∑–∞'],
            
            # –û–±—â–∏–µ —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
            '–∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞': ['–∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞', '–ø–æ–µ–∑–¥–∫–∞', '–ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ', 'business trip'],
            '–æ–±–µ–¥': ['–æ–±–µ–¥', '–ø–µ—Ä–µ—Ä—ã–≤', '–ø–∏—Ç–∞–Ω–∏–µ', '—Å—Ç–æ–ª–æ–≤–∞—è', '–∫–∞—Ñ–µ'],
            '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç': ['—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '–ø—Ä–æ–µ–∑–¥', '–∞–≤—Ç–æ–±—É—Å', '–º–∞—à–∏–Ω–∞', '—Ç–∞–∫—Å–∏'],
            '–ø–∞—Ä–∫–æ–≤–∫–∞': ['–ø–∞—Ä–∫–æ–≤–∫–∞', '—Å—Ç–æ—è–Ω–∫–∞', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', '–º–µ—Å—Ç–æ –¥–ª—è –º–∞—à–∏–Ω—ã'],
            '–ø—Ä–æ–ø—É—Å–∫': ['–ø—Ä–æ–ø—É—Å–∫', '–¥–æ—Å—Ç—É–ø', '–ø—Ä–æ—Ö–æ–¥', '–∫–∞—Ä—Ç–∞', 'badge'],
            '–¥—Ä–µ—Å—Å_–∫–æ–¥': ['–¥—Ä–µ—Å—Å –∫–æ–¥', '–æ–¥–µ–∂–¥–∞', '–≤–Ω–µ—à–Ω–∏–π –≤–∏–¥', '—Ñ–æ—Ä–º–∞', 'uniform'],
            
            # –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ –ª—å–≥–æ—Ç—ã
            '–ª—å–≥–æ—Ç—ã': ['–ª—å–≥–æ—Ç—ã', '–∫–æ–º–ø–µ–Ω—Å–∞—Ü–∏–∏', '–≤–æ–∑–º–µ—â–µ–Ω–∏–µ', 'benefits', '–ø–æ—Å–æ–±–∏—è'],
            '—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ': ['—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ', '–¥–º—Å', '–ø–æ–ª–∏—Å', '–º–µ–¥—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞'],
            '—Å–ø–æ—Ä—Ç': ['—Å–ø–æ—Ä—Ç', '—Ñ–∏—Ç–Ω–µ—Å', '—Ç—Ä–µ–Ω–∞–∂–µ—Ä–Ω—ã–π –∑–∞–ª', '–∑–¥–æ—Ä–æ–≤—å–µ', '—Ñ–∏–∑–∫—É–ª—å—Ç—É—Ä–∞'],
            '–æ–±—É—á–µ–Ω–∏–µ': ['–æ–±—É—á–µ–Ω–∏–µ', '–∫—É—Ä—Å—ã', '—Ç—Ä–µ–Ω–∏–Ω–≥–∏', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ'],
            
            # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
            '–æ—Ñ–∏—Å': ['–æ—Ñ–∏—Å', '–ø–æ–º–µ—â–µ–Ω–∏–µ', '—Ä–∞–±–æ—á–µ–µ –º–µ—Å—Ç–æ', '–∫–∞–±–∏–Ω–µ—Ç', 'space'],
            '–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ': ['–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ', '–∏–Ω–≤–µ–Ω—Ç–∞—Ä—å', '—Ç–µ—Ö–Ω–∏–∫–∞', '—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞'],
            '—É–±–æ—Ä–∫–∞': ['—É–±–æ—Ä–∫–∞', '—á–∏—Å—Ç–æ—Ç–∞', '–∫–ª–∏–Ω–∏–Ω–≥', '—Å–∞–Ω–∏—Ç–∞—Ä–∏—è', '–≥–∏–≥–∏–µ–Ω–∞'],
            '—Ä–µ–º–æ–Ω—Ç': ['—Ä–µ–º–æ–Ω—Ç', '–ø–æ–ª–æ–º–∫–∞', '–Ω–µ–∏—Å–ø—Ä–∞–≤–Ω–æ—Å—Ç—å', '—Å–µ—Ä–≤–∏—Å', 'maintenance']
        }
        
        question_lower = question.lower()
        keywords = set()
        
        # –ò—â–µ–º –ø—Ä—è–º—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏
        for base_word, word_list in synonyms.items():
            for word in word_list:
                if word in question_lower:
                    keywords.update([base_word])  # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤–æ–µ —Å–ª–æ–≤–æ
                    keywords.add(word)  # –ò —Å–∞–º–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ
                    break
        
        # –î–æ–±–∞–≤–ª—è–µ–º —á–∏—Å–ª–∞ (–¥–∞—Ç—ã, –ø—Ä–æ—Ü–µ–Ω—Ç—ã, —Å—É–º–º—ã)
        import re
        numbers = re.findall(r'\b\d{1,2}\b', question)
        for num in numbers:
            keywords.add(num)
        
        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        words = re.findall(r'\b[–∞-—è—ë]{4,}\b', question_lower)
        stop_words = {
            '–∫–æ–≥–¥–∞', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '—Å–∫–æ–ª—å–∫–æ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', 
            '–æ—Ç–∫—É–¥–∞', '–∫—É–¥–∞', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä—ã–µ', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ',
            '–¥–æ–ª–∂–µ–Ω', '–¥–æ–ª–∂–Ω–∞', '–±—É–¥–µ—Ç', '–±—ã–ª–∏', '–µ—Å—Ç—å', '–±—ã–ª–æ', '–±—É–¥—É'
        }
        
        for word in words:
            if word not in stop_words:
                keywords.add(word)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö —Å–ª–æ–≤ (–¥–ª—è IT —Ç–µ—Ä–º–∏–Ω–æ–≤)
        english_words = re.findall(r'\b[a-z]{3,}\b', question.lower())
        for word in english_words:
            if word not in ['and', 'the', 'for', 'are', 'you', 'can', 'how', 'what']:
                keywords.add(word)
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä (–°–ò–ó, –î–ú–°, IT –∏ —Ç.–¥.)
        abbreviations = re.findall(r'\b[–ê-–Ø–Å]{2,6}\b|\b[A-Z]{2,6}\b', question)
        for abbr in abbreviations:
            keywords.add(abbr.lower())
        
        self.logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {list(keywords)[:15]}")
        return list(keywords)[:15]  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–æ 15 —Å–ª–æ–≤
    
    def _fallback_search(self, question: str, limit: int) -> List[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        try:
            self.logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –∫–∞–∫ fallback")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            question_keywords = self._extract_dynamic_keywords(question)
            
            # –°–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ –≥–∏–±–∫–∏–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            search_conditions = []
            params = {'limit': limit}
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
            for i, keyword in enumerate(question_keywords[:5]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                search_conditions.append(f"dc.content ILIKE :keyword_{i}")
                params[f'keyword_{i}'] = f'%{keyword}%'
            
            if not search_conditions:
                # –ï—Å–ª–∏ –Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ —Å–ª–æ–≤–∞–º
                words = question.lower().split()
                search_terms = [word for word in words if len(word) > 2][:3]
                
                for i, term in enumerate(search_terms):
                    search_conditions.append(f"dc.content ILIKE :term_{i}")
                    params[f'term_{i}'] = f'%{term}%'
            
            if not search_conditions:
                return []
            
            # –°—Ç—Ä–æ–∏–º –∑–∞–ø—Ä–æ—Å —Å OR —É—Å–ª–æ–≤–∏—è–º–∏ –¥–ª—è –±–æ–ª—å—à–µ–π –≥–∏–±–∫–æ—Å—Ç–∏
            search_query = " OR ".join(search_conditions)
            
            query = text(f"""
                SELECT dc.id, dc.document_id, dc.chunk_index, dc.content,
                       0.6 as similarity
                FROM document_chunks dc
                JOIN documents d ON dc.document_id = d.id
                WHERE d.processing_status = 'completed'
                  AND ({search_query})
                ORDER BY 
                    CASE 
                        WHEN dc.content ILIKE :first_keyword THEN 1
                        ELSE 2
                    END,
                    LENGTH(dc.content) ASC
                LIMIT :limit
            """)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ –ø–µ—Ä–≤–æ–º—É –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
            if question_keywords:
                params['first_keyword'] = f'%{question_keywords[0]}%'
            else:
                params['first_keyword'] = f'%{question.split()[0] if question.split() else ""}%'
            
            result = self.db_session.execute(query, params)
            
            chunks = []
            for row in result:
                chunks.append({
                    'id': row.id,
                    'document_id': row.document_id,
                    'chunk_index': row.chunk_index,
                    'content': row.content,
                    'similarity': row.similarity,
                    'search_type': 'fallback_enhanced'
                })
            
            self.logger.info(f"–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞–π–¥–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {question_keywords[:5]}")
            return chunks
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ —É–ª—É—á—à–µ–Ω–Ω–æ–º fallback –ø–æ–∏—Å–∫–µ: {str(e)}")
            return []
    
    def _extract_dynamic_keywords(self, text: str) -> List[str]:
        """–î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞"""
        import re
        from collections import Counter
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        text_lower = text.lower()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ (—Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ)
        russian_words = re.findall(r'\b[–∞-—è—ë]{3,}\b', text_lower)
        english_words = re.findall(r'\b[a-z]{3,}\b', text_lower)
        abbreviations = re.findall(r'\b[–ê-–Ø–ÅA-Z]{2,6}\b', text)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–ª–æ–≤–∞
        all_words = russian_words + english_words + [abbr.lower() for abbr in abbreviations]
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            # –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫—Ç–æ', '—á–µ–π', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ',
            '—ç—Ç–æ', '—Ç–æ—Ç', '—Ç–µ–º', '—Ç–æ–º', '–æ–Ω–∏', '–æ–Ω–æ', '–æ–Ω–∞', '–µ–≥–æ', '–µ—ë', '–∏—Ö', '–¥–ª—è', '–ø—Ä–∏', '–ø–æ–¥',
            '–Ω–∞–¥', '–±–µ–∑', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É', '–ø–µ—Ä–µ–¥', '–ø–æ—Å–ª–µ', '–≤–º–µ—Å—Ç–æ', '–∫—Ä–æ–º–µ', '–±—É–¥–µ—Ç', '–±—ã–ª–∞',
            '–±—ã–ª–∏', '–±—É–¥—É', '–µ—Å—Ç—å', '–±—ã–ª', '–º–æ–∂–µ—Ç', '–º–æ–∂–Ω–æ', '–Ω—É–∂–Ω–æ', '–¥–æ–ª–∂–µ–Ω', '–¥–æ–ª–∂–Ω–∞', '–¥–æ–ª–∂–Ω—ã',
            '–æ—á–µ–Ω—å', '–≤—Å–µ—Ö', '–≤—Å–µ–º', '–≤—Å–µ–π', '–≤—Å—ë', '–≤—Å–µ', '–∏–ª–∏', '—Ç–∞–∫–∂–µ', '–µ—Å–ª–∏', '—á—Ç–æ', '—á—Ç–æ–±—ã',
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            'the', 'and', 'for', 'are', 'you', 'can', 'how', 'what', 'where', 'when', 'why', 'who',
            'this', 'that', 'they', 'them', 'with', 'from', 'have', 'will', 'been', 'were', 'was'
        }
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        meaningful_words = [word for word in all_words if word not in stop_words and len(word) > 2]
        
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∏ –±–µ—Ä–µ–º —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ
        word_counts = Counter(meaningful_words)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-10 —Å–ª–æ–≤
        return [word for word, count in word_counts.most_common(10)]
    
    def analyze_document_keywords(self, document_id: int) -> List[str]:
        """–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ç–µ–º"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            chunks_query = text("""
                SELECT content FROM document_chunks 
                WHERE document_id = :doc_id
                ORDER BY chunk_index
            """)
            
            result = self.db_session.execute(chunks_query, {'doc_id': document_id})
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            full_text = " ".join([row.content for row in result])
            
            if not full_text:
                return []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self._extract_dynamic_keywords(full_text)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—â–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
            specialized_terms = self._find_specialized_terms(full_text)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            all_keywords = list(set(keywords + specialized_terms))
            
            self.logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {all_keywords[:10]}")
            return all_keywords[:15]
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}: {str(e)}")
            return []
    
    def _find_specialized_terms(self, text: str) -> List[str]:
        """–ü–æ–∏—Å–∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        import re
        
        specialized_patterns = {
            '—Ç–µ—Ö–Ω–∏–∫–∞_–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏': [
                r'—Ç–µ—Ö–Ω–∏–∫[–∞–∏] –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏', r'–æ—Ö—Ä–∞–Ω[–∞–µ—ã] —Ç—Ä—É–¥–∞', r'–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω[—ã–π–∞—è] –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å',
                r'–Ω–µ—Å—á–∞—Å—Ç–Ω[—ã–π–æ–µ] —Å–ª—É—á–∞[–π–∏]', r'—Ç—Ä–∞–≤[–º—ã]', r'—Å—Ä–µ–¥—Å—Ç–≤–∞ –∑–∞—â–∏—Ç—ã', r'—Å–ø–µ—Ü–æ–¥–µ–∂–¥[–∞—ã]',
                r'–∏–Ω—Å—Ç—Ä—É–∫—Ç–∞–∂', r'–º–µ–¥–æ—Å–º–æ—Ç—Ä', r'–ø–æ–∂–∞—Ä–Ω[–∞—è–æ–µ] –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å'
            ],
            'it_–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å': [
                r'–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω[–∞—è–æ–µ] –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', r'–∫–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', r'–∞–Ω—Ç–∏–≤–∏—Ä—É—Å',
                r'–ø–∞—Ä–æ–ª[—å–∏]', r'–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏[—è–∏]', r'–¥–æ—Å—Ç—É–ø –∫ —Å–∏—Å—Ç–µ–º–µ', r'–∑–∞—â–∏—Ç–∞ –¥–∞–Ω–Ω—ã—Ö',
                r'—Ä–µ–∑–µ—Ä–≤–Ω[—ã–æ–µ] –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏[–µ—è]', r'—à–∏—Ñ—Ä–æ–≤–∞–Ω–∏[–µ—è]'
            ],
            '—ç–∫–æ–ª–æ–≥–∏—è': [
                r'—ç–∫–æ–ª–æ–≥–∏—á–µ—Å–∫[–∞—è–æ–µ]', r'–æ–∫—Ä—É–∂–∞—é—â[–∞—è–µ–µ] —Å—Ä–µ–¥[–∞—ã]', r'—É—Ç–∏–ª–∏–∑–∞—Ü–∏[—è–∏]',
                r'–æ—Ç—Ö–æ–¥[—ã–æ–≤]', r'–ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫[–∞–∏]', r'–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏[–µ—è]', r'–ø—Ä–∏—Ä–æ–¥–æ–æ—Ö—Ä–∞–Ω–Ω[—ã–π–∞—è]'
            ],
            '–∫–∞—á–µ—Å—Ç–≤–æ': [
                r'–∫–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞', r'—Å—Ç–∞–Ω–¥–∞—Ä—Ç[—ã–æ–≤]', r'—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏[—è–∏]',
                r'iso \d+', r'–≥–æ—Å—Ç', r'—Ç–µ—Ö–Ω–∏—á–µ—Å–∫[–∏–µ–æ] —Ç—Ä–µ–±–æ–≤–∞–Ω–∏[—è–π]'
            ]
        }
        
        found_terms = []
        text_lower = text.lower()
        
        for category, patterns in specialized_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    found_terms.append(category)
                    # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    matches = re.findall(pattern, text_lower)
                    found_terms.extend(matches[:3])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        
        return list(set(found_terms))
    
    def format_context(self, chunks: List[DocumentChunk]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        if not chunks:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞."
        
        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document = self.db_session.query(Document).filter(
                Document.id == chunk['document_id']
            ).first()
            
            doc_title = document.title if document else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"
            
            context_parts.append(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {doc_title}]\n{chunk['content']}\n"
            )
        
        return "\n".join(context_parts)
    
    def answer_question(self, 
                       question: str,
                       user_id: Optional[int] = None) -> Dict[str, Any]:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è - –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            
        Returns:
            Dict —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        try:
            self.logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å: {question[:100]}...")
            
            # 1. –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            relevant_chunks = self.search_relevant_chunks(question, limit=self.search_limit)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π –ª–∏–º–∏—Ç
            
            if not relevant_chunks:
                return {
                    'answer': '–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –Ω–∞—à–µ–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –≤–æ–ø—Ä–æ—Å—É –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ HR-–æ—Ç–¥–µ–ª—É.',
                    'sources': [],
                    'chunks': [],
                    'files': [],
                    'success': True,
                    'tokens_used': 0
                }
            
            # 2. –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ - –±–µ—Ä–µ–º –ª—É—á—à–∏–µ —á–∞–Ω–∫–∏
            top_chunks = relevant_chunks[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 –ª—É—á—à–∏—Ö —á–∞–Ω–∫–æ–≤
            context = self.format_context(top_chunks)
            
            # –û–¢–õ–ê–î–ö–ê: –í—ã–≤–æ–¥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ª–æ–≥
            self.logger.info(f"üîç –ö–û–ù–¢–ï–ö–°–¢ –î–õ–Ø LLM (–¥–ª–∏–Ω–∞: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤):")
            self.logger.info("="*80)
            self.logger.info(context[:2000] + "..." if len(context) > 2000 else context)
            self.logger.info("="*80)
            
            # 3. –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç LLM —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
            enhanced_prompt = f"""
–í–æ–ø—Ä–æ—Å: {question}

–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
1. –ë—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω—ã–º –∏ –ø–æ–¥—Ä–æ–±–Ω—ã–º
2. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
3. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Å–ø–∏—Å–∫–∞–º–∏ –≥–¥–µ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ
4. –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã, –¥–∞—Ç—ã, —Å—É–º–º—ã - –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–∫–∞–∂–∏ –∏—Ö
5. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
"""
            
            llm_response = self.llm_client.generate_answer(
                context=enhanced_prompt,
                question=question
            )
            
            if not llm_response.success:
                return {
                    'answer': '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.',
                    'sources': [],
                    'chunks': [],
                    'files': [],
                    'success': False,
                    'error': llm_response.error,
                    'tokens_used': 0
                }
            
            # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ —Ñ–∞–π–ª—ã —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π
            sources = []
            files = []
            seen_documents = set()
            
            for chunk in top_chunks:
                document = self.db_session.query(Document).filter(
                    Document.id == chunk['document_id']
                ).first()
                
                if document and document.title not in seen_documents:
                    sources.append({
                        'title': document.title,
                        'chunk_index': chunk['chunk_index'],
                        'document_id': document.id
                    })
                    
                    # –û–¢–õ–ê–î–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    self.logger.info(f"üìÑ –î–û–ö–£–ú–ï–ù–¢ ID {document.id}:")
                    self.logger.info(f"  - title: '{document.title}'")
                    self.logger.info(f"  - file_path: '{document.file_path}'")
                    self.logger.info(f"  - original_filename: '{document.original_filename}'")
                    self.logger.info(f"  - file_type: '{document.file_type}'")
                    self.logger.info(f"  - file_size: {document.file_size}")
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–µ –¥–ª—è –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–∏—è
                    files.append({
                        'title': document.title,
                        'file_path': document.file_path,  # –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                        'document_id': document.id,
                        'similarity': chunk['similarity'],
                        'file_size': document.file_size,
                        'file_type': document.file_type,
                        'original_filename': document.original_filename
                    })
                    
                    seen_documents.add(document.title)
            
            # 5. –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            formatted_answer = self._post_process_answer(llm_response.text)

            # 6. –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            if user_id:
                self._log_query(user_id, question, formatted_answer, len(relevant_chunks))
            
            return {
                'answer': formatted_answer,
                'sources': sources,
                'chunks': relevant_chunks,
                'files': files[:5],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Ñ–∞–π–ª–æ–≤
                'success': True,
                'tokens_used': llm_response.tokens_used,
                'chunks_found': len(relevant_chunks),
                'context_length': len(context)
            }
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≤ answer_question: {str(e)}")
            return {
                'answer': '–ü—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä—É.',
                'sources': [],
                'chunks': [],
                'files': [],
                'success': False,
                'error': str(e),
                'tokens_used': 0
            }
    
    def _post_process_answer(self, answer: str) -> str:
        """
        –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Args:
            answer: –ò—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM
            
        Returns:
            str: –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        answer = answer.strip()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–µ—Ä–µ–Ω–æ—Å—ã —Å—Ç—Ä–æ–∫ - —É–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –ø–µ—Ä–µ–Ω–æ—Å—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–≤–æ–π–Ω—ã–µ
        lines = answer.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            if line:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                cleaned_lines.append(line)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ–¥–Ω–∏–º –ø–µ—Ä–µ–Ω–æ—Å–æ–º –º–µ–∂–¥—É –Ω–∏–º–∏
        result = '\n'.join(cleaned_lines)
        
        # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        sentences = result.split('. ')
        unique_sentences = []
        seen_sentences = set()
        
        for sentence in sentences:
            sentence_clean = sentence.strip().lower()
            if sentence_clean and sentence_clean not in seen_sentences and len(sentence_clean) > 10:
                unique_sentences.append(sentence.strip())
                seen_sentences.add(sentence_clean)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫–∏ –≤ –∫–æ–Ω—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        final_sentences = []
        for i, sentence in enumerate(unique_sentences):
            if not sentence.endswith('.') and not sentence.endswith(':') and not sentence.endswith(';'):
                if i < len(unique_sentences) - 1:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                    sentence += '.'
            final_sentences.append(sentence)
        
        return '. '.join(final_sentences)
    
    def _log_query(self, user_id: int, question: str, answer: str, chunks_count: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            from shared.models.query_log import QueryLog
            
            log_entry = QueryLog(
                user_id=user_id,
                query_text=question,
                response_text=answer
            )
            
            self.db_session.add(log_entry)
            self.db_session.commit()
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
    
    def health_check(self) -> Dict[str, bool]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        return {
            'embeddings_model': self.embedding_model is not None,
            'llm_client': self.llm_client.health_check(),
            'database': self._check_database()
        }
    
    def _check_database(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            self.db_session.execute(text("SELECT 1"))
            return True
        except Exception:
            return False

    def _is_relevant_content(self, content: str, question: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∫ –≤–æ–ø—Ä–æ—Å—É"""
        content_lower = content.lower()
        question_lower = question.lower()
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —á–∞—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        technical_markers = [
            '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—É—Ç–≤–µ—Ä–∂–¥–∞—é', '–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä', 
            '—Å–∏—Å—Ç–µ–º–∞ –º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–∞', '–≤–≤–µ–¥–µ–Ω–æ –≤–ø–µ—Ä–≤—ã–µ', '–¥–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è',
            '–æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è', '–Ω–∞—Å—Ç–æ—è—â–µ–µ –ø–æ–ª–æ–∂–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ',
            '–∞–∫—Ü–∏–æ–Ω–µ—Ä–Ω–æ–µ –æ–±—â–µ—Å—Ç–≤–æ', '—Å–∏–±–≥–∞–∑–ø–æ–ª–∏–º–µ—Ä'
        ]
        
        # –ï—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –º–∞—Ä–∫–µ—Ä–æ–≤, –∏—Å–∫–ª—é—á–∞–µ–º
        technical_count = sum(1 for marker in technical_markers if marker in content_lower)
        if technical_count > 2:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
        question_words = set(word for word in question_lower.split() if len(word) > 2)
        content_words = set(content_lower.split())
        
        overlap = question_words & content_words
        
        # –ú–∏–Ω–∏–º—É–º 1 –æ–±—â–µ–µ —Å–ª–æ–≤–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ (—Å–Ω–∏–∂–∞–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è)
        if len(overlap) >= 1:
            return True
            
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
        key_terms = {
            '–æ—Ç–ø—É—Å–∫': ['–æ—Ç–ø—É—Å–∫', '–æ—Ç–ø—É—Å–∫–Ω—ã–µ', '–æ—Ç–¥—ã—Ö', '–æ—Ç–ø—É—Å–∫–∞', '–æ—Ç–ø—É—Å–∫–æ–≤', '–æ—Ç–ø—É—Å–∫–Ω–æ–π'],
            '–∑–∞—Ä–ø–ª–∞—Ç–∞': ['–∑–∞—Ä–ø–ª–∞—Ç–∞', '–∑–∞—Ä–∞–±–æ—Ç–Ω–∞—è –ø–ª–∞—Ç–∞', '–æ–ø–ª–∞—Ç–∞ —Ç—Ä—É–¥–∞', '–≤—ã–ø–ª–∞—á–∏–≤–∞–µ—Ç—Å—è', '–≤—ã–ø–ª–∞—Ç–∞'],
            '–≤—ã–ø–ª–∞—Ç—ã': ['–≤—ã–ø–ª–∞—Ç—ã', '–≤—ã–ø–ª–∞—Ç–∞', '–Ω–∞—á–∏—Å–ª–µ–Ω–∏—è', '–ø—Ä–µ–º–∏—è'],
            '—é–±–∏–ª–µ–π': ['—é–±–∏–ª–µ–π', '—é–±–∏–ª–µ–π–Ω—ã–µ', '–≥–æ–¥–æ–≤—â–∏–Ω–∞'],
            '–±–æ–ª—å–Ω–∏—á–Ω—ã–π': ['–±–æ–ª—å–Ω–∏—á–Ω—ã–π', '–Ω–µ—Ç—Ä—É–¥–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å'],
            '–∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞': ['–∫–æ–º–∞–Ω–¥–∏—Ä–æ–≤–∫–∞', '—Å–ª—É–∂–µ–±–Ω–∞—è –ø–æ–µ–∑–¥–∫–∞'],
            '—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ': ['—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ', '—Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞'],
            '–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ': ['–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ', '–æ—Ñ–æ—Ä–º–∏—Ç—å', '–æ—Ñ–æ—Ä–º–ª–µ–Ω–∏—è', '–ø—Ä–æ—Ü–µ–¥—É—Ä–∞', '–ø–æ—Ä—è–¥–æ–∫']
        }
        
        for term_group in key_terms.values():
            question_has_term = any(term in question_lower for term in term_group)
            content_has_term = any(term in content_lower for term in term_group)
            
            if question_has_term and content_has_term:
                return True
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º False
            return False 